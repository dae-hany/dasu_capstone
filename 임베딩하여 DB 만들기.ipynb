{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ì»¤ë¦¬í˜ëŸ¼ ë°ì´í„° ì„ë² ë”©í•˜ì—¬ db ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "years = [2021, 2022, 2023, 2024, 2025]\n",
    "\n",
    "df_ì¸ê³µì§€ëŠ¥í•™ë¶€ = list()\n",
    "df_ì¸ê³µì§€ëŠ¥ì „ê³µ = list()\n",
    "df_ì†Œí”„íŠ¸ì›¨ì–´ì „ê³µ = list()\n",
    "\n",
    "for year in years:\n",
    "    df1 = pd.read_csv(f\"{year} ì¸ê³µì§€ëŠ¥í•™ë¶€ ì»¤ë¦¬í˜ëŸ¼.csv\")\n",
    "    df_ì¸ê³µì§€ëŠ¥í•™ë¶€.append(df1)\n",
    "\n",
    "    df2 = pd.read_csv(f\"{year} ì¸ê³µì§€ëŠ¥ì „ê³µ ì»¤ë¦¬í˜ëŸ¼.csv\")\n",
    "    df_ì¸ê³µì§€ëŠ¥ì „ê³µ.append(df2)\n",
    "\n",
    "    df3 = pd.read_csv(f\"{year} ì†Œí”„íŠ¸ì›¨ì–´ì „ê³µ ì»¤ë¦¬í˜ëŸ¼.csv\")\n",
    "    df_ì†Œí”„íŠ¸ì›¨ì–´ì „ê³µ.append(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv(\"2025 ì •ë³´ë³´ì•ˆì „ê³µ ì»¤ë¦¬í˜ëŸ¼.csv\")\n",
    "df5 = pd.read_csv(\"2024 ì •ë³´ë³´ì•ˆì „ê³µ ì»¤ë¦¬í˜ëŸ¼.csv\")\n",
    "\n",
    "df_ì •ë³´ë³´ì•ˆì „ê³µ = [df4, df5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding_text(df):\n",
    "    df['ì„ë² ë”© í…ìŠ¤íŠ¸'] = df['êµê³¼ëª©ëª…'] + \" (\" + df['í•™ë…„'] + \" \" + df['í•™ê¸°'] + \", \" + df['ê³¼ëª©êµ¬ë¶„'] + \")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_curriculum = df_ì¸ê³µì§€ëŠ¥í•™ë¶€ + df_ì¸ê³µì§€ëŠ¥ì „ê³µ + df_ì†Œí”„íŠ¸ì›¨ì–´ì „ê³µ + df_ì •ë³´ë³´ì•ˆì „ê³µ\n",
    "len(df_curriculum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_curriculum:\n",
    "    make_embedding_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ì„ë² ë”© ìƒì„±\n",
    "for i, df in enumerate(df_curriculum):\n",
    "    embeddings = model.encode(df['ì„ë² ë”© í…ìŠ¤íŠ¸'].tolist(), convert_to_tensor=True)\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(np.array(embeddings.cpu()))  # FAISSì— ë²¡í„° ì¶”ê°€\n",
    "    faiss.write_index(index, f\"curriculum_faiss_{i+1}.index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì§ˆë¬¸ ë°›ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ğŸ”¹ Step 1: ì‚¬ì „ ì¤€ë¹„ (ëª¨ë¸ ë° ì¸ë±ìŠ¤ ë¡œë“œ)\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ğŸ”¹ Step 2: FAISS ì¸ë±ìŠ¤ íŒŒì¼ ë¡œë“œ\n",
    "index_files = [\n",
    "    \"curriculum_faiss_1.index\", \"curriculum_faiss_2.index\", \"curriculum_faiss_3.index\",\n",
    "    \"curriculum_faiss_4.index\", \"curriculum_faiss_5.index\", \"curriculum_faiss_6.index\",\n",
    "    \"curriculum_faiss_7.index\", \"curriculum_faiss_8.index\", \"curriculum_faiss_9.index\",\n",
    "    \"curriculum_faiss_10.index\", \"curriculum_faiss_11.index\", \"curriculum_faiss_12.index\",\n",
    "    \"curriculum_faiss_13.index\", \"curriculum_faiss_14.index\", \"curriculum_faiss_15.index\",\n",
    "    \"curriculum_faiss_16.index\", \"curriculum_faiss_17.index\"\n",
    "]\n",
    "\n",
    "indexes = [faiss.read_index(f) for f in index_files]  # ì—¬ëŸ¬ ê°œì˜ FAISS ì¸ë±ìŠ¤ ë¡œë“œ\n",
    "index = faiss.IndexFlatL2(indexes[0].d)  # ìƒˆë¡œìš´ í†µí•© ì¸ë±ìŠ¤ ìƒì„±\n",
    "index = faiss.IndexIDMap(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¹ Step 3: ì›ë³¸ ì»¤ë¦¬í˜ëŸ¼ ë°ì´í„° ë¡œë“œ (ë¬¸ì„œ ID â†’ êµê³¼ëª© ë§¤ì¹­)\n",
    "df_files = [\n",
    "    \"2021 ì¸ê³µì§€ëŠ¥í•™ë¶€ ì»¤ë¦¬í˜ëŸ¼.csv\", \"2022 ì¸ê³µì§€ëŠ¥í•™ë¶€ ì»¤ë¦¬í˜ëŸ¼.csv\", \"2023 ì¸ê³µì§€ëŠ¥í•™ë¶€ ì»¤ë¦¬í˜ëŸ¼.csv\", \"2024 ì¸ê³µì§€ëŠ¥í•™ë¶€ ì»¤ë¦¬í˜ëŸ¼.csv\", \"2025 ì¸ê³µì§€ëŠ¥í•™ë¶€ ì»¤ë¦¬í˜ëŸ¼.csv\",\n",
    "    \"2021 ì¸ê³µì§€ëŠ¥ì „ê³µ ì»¤ë¦¬í˜ëŸ¼.csv\", \"2022 ì¸ê³µì§€ëŠ¥ì „ê³µ ì»¤ë¦¬í˜ëŸ¼.csv\", \"2023 ì¸ê³µì§€ëŠ¥ì „ê³µ ì»¤ë¦¬í˜ëŸ¼.csv\", \"2024 ì¸ê³µì§€ëŠ¥ì „ê³µ ì»¤ë¦¬í˜ëŸ¼.csv\", \"2025 ì¸ê³µì§€ëŠ¥ì „ê³µ ì»¤ë¦¬í˜ëŸ¼.csv\",\n",
    "    \"2021 ì†Œí”„íŠ¸ì›¨ì–´ì „ê³µ ì»¤ë¦¬í˜ëŸ¼.csv\", \"2022 ì†Œí”„íŠ¸ì›¨ì–´ì „ê³µ ì»¤ë¦¬í˜ëŸ¼.csv\", \"2023 ì†Œí”„íŠ¸ì›¨ì–´ì „ê³µ ì»¤ë¦¬í˜ëŸ¼.csv\", \"2024 ì†Œí”„íŠ¸ì›¨ì–´ì „ê³µ ì»¤ë¦¬í˜ëŸ¼.csv\", \"2025 ì†Œí”„íŠ¸ì›¨ì–´ì „ê³µ ì»¤ë¦¬í˜ëŸ¼.csv\",\n",
    "    \"2024 ì •ë³´ë³´ì•ˆì „ê³µ ì»¤ë¦¬í˜ëŸ¼.csv\", \"2025 ì •ë³´ë³´ì•ˆì „ê³µ ì»¤ë¦¬í˜ëŸ¼.csv\"\n",
    "]\n",
    "df_curriculum = [pd.read_csv(f) for f in df_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¹ Step 4: ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜\n",
    "def search_faiss(query: str, top_k: int = 5):\n",
    "    \"\"\"FAISSì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ì»¤ë¦¬í˜ëŸ¼ì„ ê²€ìƒ‰í•˜ì—¬ ë°˜í™˜\"\"\"\n",
    "    query_vector = embedding_model.encode(query).astype(np.float32).reshape(1, -1)  # ì¿¼ë¦¬ ë²¡í„° ë³€í™˜\n",
    "    D, I = index.search(query_vector, top_k)  # FAISSì—ì„œ ê²€ìƒ‰\n",
    "    \n",
    "    results = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        results.append((idx, score))  # (ë¬¸ì„œ ID, ìœ ì‚¬ë„ ì ìˆ˜)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-1, 3.4028235e+38),\n",
       " (-1, 3.4028235e+38),\n",
       " (-1, 3.4028235e+38),\n",
       " (-1, 3.4028235e+38),\n",
       " (-1, 3.4028235e+38)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_faiss(\"ì¸ê³µì§€ëŠ¥ êµê³¼ëª© ìˆ˜ê°• ì‹ ì²­\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¹ Step 3: ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ğŸ”¹ Step 4: ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜\n",
    "def search_faiss(query: str, top_k: int = 5):\n",
    "    \"\"\"FAISS ì¸ë±ìŠ¤ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰\"\"\"\n",
    "    query_vector = embedding_model.encode(query).astype(np.float32).reshape(1, -1)  # ì¿¼ë¦¬ ë²¡í„° ë³€í™˜\n",
    "    D, I = index.search(query_vector, top_k)  # FAISSì—ì„œ ê²€ìƒ‰\n",
    "    \n",
    "    results = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        if idx != -1:\n",
    "            results.append((idx, score))  # (ë¬¸ì„œ ID, ìœ ì‚¬ë„ ì ìˆ˜)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama \n",
    "\n",
    "# ğŸ”¹ Step 6: Ollama ê¸°ë°˜ LLM ì±—ë´‡\n",
    "def rag_ollama_chat(query: str, top_k: int = 5):\n",
    "    \"\"\"ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ FAISSë¡œ ê²€ìƒ‰í•˜ê³  Ollamaê°€ ì‘ë‹µ\"\"\"\n",
    "\n",
    "    # ğŸ”¹ Step 6-1: FAISSì—ì„œ ê²€ìƒ‰\n",
    "    search_results = search_faiss(query, top_k)\n",
    "\n",
    "    if not search_results:\n",
    "        return \"ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì»¤ë¦¬í˜ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "    # ğŸ”¹ Step 6-2: ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    retrieved_docs = []\n",
    "    for doc_id, score in search_results:\n",
    "        for df in df_curriculum:\n",
    "            if doc_id < len(df):  # ë¬¸ì„œ IDê°€ ìœ íš¨í•œ ê²½ìš°\n",
    "                subject_info = df.iloc[doc_id]  # í•´ë‹¹ êµê³¼ëª© ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "                doc_text = (\n",
    "                    f\"- {subject_info['êµê³¼ëª©ëª…']} ({subject_info['í•™ë…„']}í•™ë…„ {subject_info['í•™ê¸°']}í•™ê¸°, {subject_info['ê³¼ëª©êµ¬ë¶„']})\"\n",
    "                )\n",
    "                retrieved_docs.append(doc_text)\n",
    "\n",
    "    # ğŸ”¹ Step 6-3: LLM í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "    retrieved_text = \"\\n\".join(retrieved_docs)\n",
    "    prompt = f\"\"\"\n",
    "    ğŸ” ë‹¤ìŒ êµê³¼ëª© ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”:\n",
    "\n",
    "    {retrieved_text}\n",
    "\n",
    "    ì§ˆë¬¸: {query}\n",
    "    \"\"\"\n",
    "\n",
    "    # ğŸ”¹ Step 6-4: Ollama LLM í˜¸ì¶œ\n",
    "    response = ollama.chat(model=\"tinyllama\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    \n",
    "    return response[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama RAG ì±—ë´‡ ì‘ë‹µ:\n",
      "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì»¤ë¦¬í˜ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ Step 6: ì±—ë´‡ í…ŒìŠ¤íŠ¸\n",
    "query_text = \"ì¸ê³µì§€ëŠ¥í•™ë¶€ ë”¥ëŸ¬ë‹ ê´€ë ¨ ê³¼ëª©ì„ ì•Œë ¤ì¤˜\"\n",
    "response = rag_ollama_chat(query_text)\n",
    "\n",
    "print(\"Ollama RAG ì±—ë´‡ ì‘ë‹µ:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dasu_capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
